{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🤖 Cyberdyne LLM - Complete Training & Inference System\n\nA complete system for training and deploying custom language models with Llama3-inspired architecture (RMSNorm, SwiGLU, RoPE).\n\n## Features\n- 🎓 Train custom language models from scratch\n- 📊 Multi-dataset support (9+ Hugging Face datasets)\n- 💾 Optimized for Kaggle's 50GB SSD with intelligent caching\n- 🚀 Gradient accumulation for effective large batch training\n- 🔄 Checkpoint resuming for interrupted training\n- 💬 Offline inference\n\n## Quick Start Guide\n1. Run the installation cell\n2. Configure dataset caching for optimal disk usage\n3. Choose to either train a new model or load an existing one\n4. Load and test your trained model\n\n**Compatible with Google Colab & Kaggle (Optimized for 50GB SSD)**","metadata":{}},{"cell_type":"markdown","source":"## 📦 Installation & Setup","metadata":{}},{"cell_type":"code","source":"# Install required packages\n!pip install -q torch>=2.0.0 transformers>=4.30.0 datasets>=2.14.0 tqdm>=4.65.0 huggingface_hub>=0.16.0 accelerate>=0.20.0 psutil\n\nprint(\"✅ All packages installed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:25:09.939898Z","iopub.execute_input":"2025-10-25T04:25:09.940213Z","iopub.status.idle":"2025-10-25T04:26:34.059282Z","shell.execute_reply.started":"2025-10-25T04:25:09.940186Z","shell.execute_reply":"2025-10-25T04:26:34.058459Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m✅ All packages installed successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Import libraries\nimport torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, IterableDataset\nfrom transformers import GPT2Tokenizer\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nimport time\nimport os\nimport math\nimport uuid\nimport shutil\nimport psutil\nimport gc\nfrom pathlib import Path\nfrom datetime import datetime\nfrom typing import Optional, Dict, List\nimport json\n\n# Check device\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"🖥️  Using device: {device}\")\nif device == 'cuda':\n    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n    # Check if multiple GPUs available\n    n_gpus = torch.cuda.device_count()\n    print(f\"   Number of GPUs: {n_gpus}\")\n    if n_gpus > 1:\n        print(\"   Multi-GPU training available!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:34.060821Z","iopub.execute_input":"2025-10-25T04:26:34.061088Z","iopub.status.idle":"2025-10-25T04:26:41.798890Z","shell.execute_reply.started":"2025-10-25T04:26:34.061065Z","shell.execute_reply":"2025-10-25T04:26:41.798134Z"}},"outputs":[{"name":"stdout","text":"🖥️  Using device: cuda\n   GPU: Tesla T4\n   Memory: 15.83 GB\n   Number of GPUs: 2\n   Multi-GPU training available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## 💾 Dataset Configuration & Disk Space Management\n\nOptimized configuration for Kaggle's 50GB SSD space","metadata":{}},{"cell_type":"code","source":"# Dataset Configuration for Kaggle Environment\nimport os\nimport shutil\nfrom pathlib import Path\nimport psutil\n\n# Kaggle-specific paths\nKAGGLE_CACHE_DIR = \"/kaggle/working/cache\"\nKAGGLE_MODEL_DIR = \"/kaggle/working/models\"\nKAGGLE_DATA_DIR = \"/kaggle/working/data\"\nKAGGLE_CHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\n\n# Create directories\nfor dir_path in [KAGGLE_CACHE_DIR, KAGGLE_MODEL_DIR, KAGGLE_DATA_DIR, KAGGLE_CHECKPOINT_DIR]:\n    os.makedirs(dir_path, exist_ok=True)\n\n# Disk space utilities\ndef get_disk_usage(path=\"/kaggle/working\"):\n    \"\"\"Get disk usage statistics for the specified path.\"\"\"\n    if os.path.exists(path):\n        total, used, free = shutil.disk_usage(path)\n    else:\n        # Fallback to current directory if Kaggle path doesn't exist\n        total, used, free = shutil.disk_usage(\".\")\n    \n    return {\n        \"total_gb\": total / (1024**3),\n        \"used_gb\": used / (1024**3), \n        \"free_gb\": free / (1024**3),\n        \"percent_used\": (used / total) * 100 if total > 0 else 0\n    }\n\ndef get_memory_usage():\n    \"\"\"Get current memory usage.\"\"\"\n    memory = psutil.virtual_memory()\n    return {\n        \"total_gb\": memory.total / (1024**3),\n        \"available_gb\": memory.available / (1024**3),\n        \"used_gb\": memory.used / (1024**3),\n        \"percent_used\": memory.percent\n    }\n\ndef monitor_resources():\n    \"\"\"Monitor disk and memory resources.\"\"\"\n    disk_info = get_disk_usage()\n    mem_info = get_memory_usage()\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"📊 Resource Monitor\")\n    print(\"=\"*60)\n    print(f\"💾 Disk: {disk_info['free_gb']:.1f}GB free of {disk_info['total_gb']:.1f}GB ({disk_info['percent_used']:.1f}% used)\")\n    print(f\"🧠 RAM: {mem_info['available_gb']:.1f}GB free of {mem_info['total_gb']:.1f}GB ({mem_info['percent_used']:.1f}% used)\")\n    \n    if device == 'cuda':\n        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n        gpu_used = torch.cuda.memory_allocated() / 1e9\n        gpu_cached = torch.cuda.memory_reserved() / 1e9\n        print(f\"🎮 GPU: {gpu_used:.1f}GB used, {gpu_cached:.1f}GB cached of {gpu_memory:.1f}GB\")\n    print(\"=\"*60 + \"\\n\")\n    \n    return disk_info, mem_info\n\n# Calculate optimal data configuration\ndisk_info = get_disk_usage()\nprint(\"\\n🚀 Kaggle Environment Configuration\")\nprint(\"=\"*60)\nprint(f\"💾 Disk space: {disk_info['free_gb']:.1f}GB free of {disk_info['total_gb']:.1f}GB\")\n\n# Reserve space for models and overhead\nDATA_CACHE_SIZE_GB = min(35, disk_info['free_gb'] - 10)  # Reserve 10GB for models/checkpoints/overhead\nMAX_SAMPLES_ESTIMATE = int(DATA_CACHE_SIZE_GB * 1e9 / 2000)  # Estimate ~2KB per tokenized sample\n\nprint(f\"📦 Will use up to {DATA_CACHE_SIZE_GB:.1f}GB for data caching\")\nprint(f\"📈 Estimated capacity: {MAX_SAMPLES_ESTIMATE:,} samples\")\nprint(\"=\"*60)\n\n# Configuration for different training scales\nTRAINING_CONFIGS = {\n    'quick': {\n        'max_samples': min(1_000_000, MAX_SAMPLES_ESTIMATE // 20),\n        'batch_size': 8,\n        'gradient_accumulation_steps': 4,\n        'checkpoint_interval': 500,\n        'description': 'Quick training with 1M samples'\n    },\n    'standard': {\n        'max_samples': min(5_000_000, MAX_SAMPLES_ESTIMATE // 4),\n        'batch_size': 8,\n        'gradient_accumulation_steps': 8,\n        'checkpoint_interval': 1000,\n        'description': 'Standard training with 5M samples'\n    },\n    'large': {\n        'max_samples': min(20_000_000, MAX_SAMPLES_ESTIMATE),\n        'batch_size': 4,\n        'gradient_accumulation_steps': 16,\n        'checkpoint_interval': 2000,\n        'description': 'Large-scale training with up to 20M samples'\n    },\n    'pile': {\n        'max_samples': MAX_SAMPLES_ESTIMATE,\n        'batch_size': 4,\n        'gradient_accumulation_steps': 32,\n        'checkpoint_interval': 5000,\n        'description': f'Pile dataset training with {MAX_SAMPLES_ESTIMATE:,} samples'\n    }\n}\n\nprint(\"\\n📋 Available Training Configurations:\")\nfor config_name, config in TRAINING_CONFIGS.items():\n    print(f\"  • {config_name}: {config['description']}\")\n    print(f\"    - Batch size: {config['batch_size']} × {config['gradient_accumulation_steps']} accumulation\")\n    print(f\"    - Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:41.799709Z","iopub.execute_input":"2025-10-25T04:26:41.800182Z","iopub.status.idle":"2025-10-25T04:26:41.815985Z","shell.execute_reply.started":"2025-10-25T04:26:41.800160Z","shell.execute_reply":"2025-10-25T04:26:41.815117Z"}},"outputs":[{"name":"stdout","text":"\n🚀 Kaggle Environment Configuration\n============================================================\n💾 Disk space: 19.5GB free of 19.5GB\n📦 Will use up to 9.5GB for data caching\n📈 Estimated capacity: 4,751,216 samples\n============================================================\n\n📋 Available Training Configurations:\n  • quick: Quick training with 1M samples\n    - Batch size: 8 × 4 accumulation\n    - Effective batch size: 32\n  • standard: Standard training with 5M samples\n    - Batch size: 8 × 8 accumulation\n    - Effective batch size: 64\n  • large: Large-scale training with up to 20M samples\n    - Batch size: 4 × 16 accumulation\n    - Effective batch size: 64\n  • pile: Pile dataset training with 4,751,216 samples\n    - Batch size: 4 × 32 accumulation\n    - Effective batch size: 128\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## 🏗️ Model Architecture\n\nLlama3-inspired Transformer with RMSNorm, SwiGLU, and Rotary Position Embeddings","metadata":{}},{"cell_type":"code","source":"# ==================== RMSNorm Implementation ====================\nclass RMSNorm(nn.Module):\n    \"\"\"Root Mean Square Layer Normalization.\n    \n    As used in Llama3, RMSNorm is computationally simpler than LayerNorm.\n    It normalizes by the root mean square without subtracting mean.\n    \"\"\"\n    def __init__(self, dim: int, eps: float = 1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(dim))\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        norm = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        return x / norm * self.weight\n\n\n# ==================== Rotary Position Embeddings (RoPE) ====================\ndef precompute_freqs_cis(dim: int, max_len: int, theta: float = 10000.0):\n    \"\"\"Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\"\"\"\n    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n    t = torch.arange(max_len)\n    freqs = torch.outer(t, freqs).float()\n    freqs_cos = torch.cos(freqs)\n    freqs_sin = torch.sin(freqs)\n    return freqs_cos, freqs_sin\n\n\ndef apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cos: torch.Tensor, freqs_sin: torch.Tensor) -> tuple:\n    \"\"\"Apply rotary embeddings to query and key tensors.\"\"\"\n    # Reshape for rotary computation\n    xq_r = xq.float().reshape(*xq.shape[:-1], -1, 2)\n    xk_r = xk.float().reshape(*xk.shape[:-1], -1, 2)\n    \n    # Apply rotation\n    xq_out_r = xq_r[..., 0] * freqs_cos - xq_r[..., 1] * freqs_sin\n    xq_out_i = xq_r[..., 0] * freqs_sin + xq_r[..., 1] * freqs_cos\n    xk_out_r = xk_r[..., 0] * freqs_cos - xk_r[..., 1] * freqs_sin\n    xk_out_i = xk_r[..., 0] * freqs_sin + xk_r[..., 1] * freqs_cos\n    \n    # Concatenate back\n    xq_out = torch.stack([xq_out_r, xq_out_i], dim=-1).flatten(-2)\n    xk_out = torch.stack([xk_out_r, xk_out_i], dim=-1).flatten(-2)\n    \n    return xq_out.type_as(xq), xk_out.type_as(xk)\n\n\n# ==================== SwiGLU Activation Function ====================\nclass SwiGLU(nn.Module):\n    \"\"\"SwiGLU activation function as used in Llama3.\n    \n    Combines Swish activation with Gated Linear Unit.\n    More effective than ReLU/GELU for language modeling.\n    \"\"\"\n    def __init__(self, dim: int, hidden_dim: Optional[int] = None):\n        super().__init__()\n        if hidden_dim is None:\n            hidden_dim = int(2 * dim * 4 / 3)\n            # Round to nearest multiple of 64 for hardware efficiency\n            hidden_dim = ((hidden_dim + 63) // 64) * 64\n        \n        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        return self.w2(nn.functional.silu(self.w1(x)) * self.w3(x))\n\n\n# ==================== Multi-Head Attention with RoPE ====================\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, emb_size, n_heads, max_len=512, dropout=0.1):\n        super().__init__()\n        assert emb_size % n_heads == 0\n        \n        self.emb_size = emb_size\n        self.n_heads = n_heads\n        self.head_dim = emb_size // n_heads\n        \n        self.qkv = nn.Linear(emb_size, 3 * emb_size, bias=False)\n        self.out = nn.Linear(emb_size, emb_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Precompute RoPE frequencies\n        self.freqs_cos, self.freqs_sin = precompute_freqs_cis(self.head_dim, max_len)\n        \n    def forward(self, x, mask=None):\n        batch_size, seq_len, _ = x.shape\n        device = x.device\n        \n        # Compute Q, K, V\n        qkv = self.qkv(x)\n        qkv = qkv.reshape(batch_size, seq_len, 3, self.n_heads, self.head_dim)\n        qkv = qkv.permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        # Apply RoPE to Q and K\n        self.freqs_cos = self.freqs_cos.to(device)\n        self.freqs_sin = self.freqs_sin.to(device)\n        freqs_cos = self.freqs_cos[:seq_len].unsqueeze(0).unsqueeze(0)\n        freqs_sin = self.freqs_sin[:seq_len].unsqueeze(0).unsqueeze(0)\n        \n        q, k = apply_rotary_emb(q, k, freqs_cos, freqs_sin)\n        \n        # Compute attention scores\n        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)\n        \n        # Create causal mask for autoregressive generation\n        if mask is None:\n            mask = torch.tril(torch.ones(seq_len, seq_len)).to(device)\n        \n        # Apply the mask to attention scores\n        attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n        \n        \n        attn_weights = torch.softmax(attn_scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        \n        # Apply attention to values\n        out = torch.matmul(attn_weights, v)\n        out = out.permute(0, 2, 1, 3).contiguous()\n        out = out.reshape(batch_size, seq_len, self.emb_size)\n        out = self.out(out)\n        \n        return out\n\n\n# ==================== Transformer Block with RMSNorm and SwiGLU ====================\nclass TransformerBlock(nn.Module):\n    def __init__(self, emb_size, n_heads, ff_size, max_len=512, dropout=0.1, use_checkpoint=False):\n        super().__init__()\n        self.attn = MultiHeadAttention(emb_size, n_heads, max_len, dropout)\n        self.ff = SwiGLU(emb_size)  # Using SwiGLU instead of standard FeedForward\n        self.ln1 = RMSNorm(emb_size)  # Using RMSNorm instead of LayerNorm\n        self.ln2 = RMSNorm(emb_size)  # Using RMSNorm instead of LayerNorm\n        self.dropout = nn.Dropout(dropout)\n        self.use_checkpoint = use_checkpoint\n    \n    def forward(self, x, mask=None):\n        if self.use_checkpoint and self.training:\n            # Use gradient checkpointing to save memory\n            return torch.utils.checkpoint.checkpoint(self._forward, x, mask)\n        else:\n            return self._forward(x, mask)\n    \n    def _forward(self, x, mask=None):\n        # Pre-norm with RMSNorm -> Attention with residual\n        attn_out = self.attn(self.ln1(x), mask)\n        x = x + self.dropout(attn_out)\n        # Pre-norm with RMSNorm -> SwiGLU FFN with residual\n        ff_out = self.ff(self.ln2(x))\n        x = x + self.dropout(ff_out)\n        return x\n\n\n# ==================== Advanced LLM with Llama3-Inspired Architecture ====================\nclass AdvancedLLM(nn.Module):\n    \"\"\"Advanced LLM with Llama3-inspired improvements:\n    - RMSNorm for efficient normalization\n    - SwiGLU activation for better performance\n    - Rotary Position Embeddings (RoPE) for better position encoding\n    - Optional gradient checkpointing for memory efficiency\n    \"\"\"\n    def __init__(self, vocab_size, emb_size=768, n_layers=12, n_heads=12,\n                 ff_size=3072, max_len=512, dropout=0.1, use_gradient_checkpoint=False):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.emb_size = emb_size\n        self.n_layers = n_layers\n        self.n_heads = n_heads\n        self.max_len = max_len\n        self.use_gradient_checkpoint = use_gradient_checkpoint\n        \n        # Token embeddings (no position embeddings needed with RoPE)\n        self.token_embed = nn.Embedding(vocab_size, emb_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Transformer blocks with RMSNorm and SwiGLU\n        self.blocks = nn.ModuleList([\n            TransformerBlock(emb_size, n_heads, ff_size, max_len, dropout, use_gradient_checkpoint)\n            for _ in range(n_layers)\n        ])\n        \n        # Final RMSNorm and output projection\n        self.ln_final = RMSNorm(emb_size)\n        self.head = nn.Linear(emb_size, vocab_size, bias=False)\n        \n        # Weight tying\n        self.token_embed.weight = self.head.weight\n        \n        self._init_weights()\n    \n    def _init_weights(self):\n        \"\"\"Initialize model weights.\"\"\"\n        for module in self.modules():\n            if isinstance(module, nn.Linear):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n                if module.bias is not None:\n                    torch.nn.init.zeros_(module.bias)\n            elif isinstance(module, nn.Embedding):\n                torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    \n    def forward(self, x, mask=None):\n        batch_size, seq_len = x.shape\n        \n        # Token embeddings (no position embeddings, using RoPE instead)\n        token_emb = self.token_embed(x)\n        x = self.dropout(token_emb)\n        \n        # Apply transformer blocks\n        for block in self.blocks:\n            x = block(x, mask)\n        \n        # Final norm and output projection\n        x = self.ln_final(x)\n        logits = self.head(x)\n        \n        return logits\n    \n    def get_num_params(self, non_embedding=False):\n        \"\"\"Count model parameters.\"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.token_embed.weight.numel()\n        return n_params\n    \n    def estimate_memory_usage(self, batch_size, seq_length):\n        \"\"\"Estimate memory usage for given batch size and sequence length.\"\"\"\n        # Rough estimation in GB\n        params_memory = self.get_num_params() * 4 / 1e9  # 4 bytes per float32\n        activation_memory = batch_size * seq_length * self.emb_size * self.n_layers * 4 / 1e9\n        gradient_memory = params_memory if self.training else 0\n        total_memory = params_memory + activation_memory + gradient_memory\n        return {\n            'params_gb': params_memory,\n            'activations_gb': activation_memory,\n            'gradients_gb': gradient_memory,\n            'total_gb': total_memory\n        }\n    \n    def save_model(self, path):\n        \"\"\"Save model to disk.\"\"\"\n        config = {\n            'vocab_size': self.vocab_size,\n            'emb_size': self.emb_size,\n            'n_layers': self.n_layers,\n            'n_heads': self.n_heads,\n            'max_len': self.max_len,\n            'use_gradient_checkpoint': self.use_gradient_checkpoint,\n            'state_dict': self.state_dict()\n        }\n        torch.save(config, path)\n        file_size = os.path.getsize(path) / 1e9\n        print(f\"✅ Model saved to {path} ({file_size:.2f}GB)\")\n    \n    @classmethod\n    def load_model(cls, path, device='cpu'):\n        \"\"\"Load model from disk.\"\"\"\n        config = torch.load(path, map_location=device)\n        model = cls(\n            vocab_size=config['vocab_size'],\n            emb_size=config['emb_size'],\n            n_layers=config['n_layers'],\n            n_heads=config['n_heads'],\n            max_len=config['max_len'],\n            use_gradient_checkpoint=config.get('use_gradient_checkpoint', False)\n        )\n        model.load_state_dict(config['state_dict'])\n        model.to(device)\n        print(f\"✅ Model loaded from {path}\")\n        return model\n\nprint(\"✅ Llama3-inspired model architecture defined\")\nprint(\"   Features: RMSNorm, SwiGLU activation, Rotary Position Embeddings (RoPE), Gradient Checkpointing\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:41.816965Z","iopub.execute_input":"2025-10-25T04:26:41.817233Z","iopub.status.idle":"2025-10-25T04:26:41.850934Z","shell.execute_reply.started":"2025-10-25T04:26:41.817213Z","shell.execute_reply":"2025-10-25T04:26:41.850162Z"}},"outputs":[{"name":"stdout","text":"✅ Llama3-inspired model architecture defined\n   Features: RMSNorm, SwiGLU activation, Rotary Position Embeddings (RoPE), Gradient Checkpointing\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## 📚 Enhanced Dataset Loader with Caching\n\nOptimized for large-scale data loading with local disk caching","metadata":{}},{"cell_type":"code","source":"class HuggingFaceDataset(IterableDataset):\n    \"\"\"Enhanced dataset loader with disk caching support for efficient large-scale training.\"\"\"\n    \n    def __init__(self, dataset_name, tokenizer, max_samples=100000,\n                 max_len=512, split='train', config=None, streaming=True,\n                 text_field='text', instruction_field=None,\n                 cache_dir=None, use_disk_cache=True, prefetch_batches=10):\n        \n        self.dataset_name = dataset_name\n        self.tokenizer = tokenizer\n        self.max_samples = max_samples\n        self.max_len = max_len\n        self.split = split\n        self.config = config\n        self.streaming = streaming\n        self.text_field = text_field\n        self.instruction_field = instruction_field\n        self.prefetch_batches = prefetch_batches\n        \n        # Caching configuration\n        self.cache_dir = cache_dir or KAGGLE_CACHE_DIR\n        self.use_disk_cache = use_disk_cache\n        \n        # Set up caching for datasets library\n        if use_disk_cache:\n            os.environ['HF_DATASETS_CACHE'] = self.cache_dir\n            os.environ['TRANSFORMERS_CACHE'] = self.cache_dir\n            os.makedirs(self.cache_dir, exist_ok=True)\n            print(f\"💾 Using cache directory: {self.cache_dir}\")\n\n        try:\n            # Load dataset with caching\n            if config:\n                self.dataset = load_dataset(\n                    dataset_name, config, split=split, \n                    streaming=streaming,\n                    cache_dir=self.cache_dir if use_disk_cache else None,\n                    keep_in_memory=False  # Don't keep entire dataset in RAM\n                )\n            else:\n                self.dataset = load_dataset(\n                    dataset_name, split=split, \n                    streaming=streaming,\n                    cache_dir=self.cache_dir if use_disk_cache else None,\n                    keep_in_memory=False\n                )\n            print(f\"✅ Loaded dataset: {dataset_name}\")\n            \n            # For large datasets, optionally take a subset\n            if streaming and max_samples is not None:\n                self.dataset = self.dataset.take(max_samples)\n                \n        except Exception as e:\n            print(f\"❌ Error loading dataset {dataset_name}: {e}\")\n            raise\n    \n    def __iter__(self):\n        count = 0\n        buffer = []\n        \n        for item in self.dataset:\n            if count >= self.max_samples:\n                break\n\n            text = self._extract_text(item)\n            if not text:\n                continue\n\n            tokens = self.tokenizer.encode(\n                text,\n                max_length=self.max_len,\n                truncation=True,\n                padding='max_length',\n                return_tensors='pt'\n            ).squeeze(0)\n\n            target = tokens.clone()\n            target[:-1] = tokens[1:]\n            target[-1] = self.tokenizer.eos_token_id or self.tokenizer.pad_token_id\n\n            buffer.append((tokens, target))\n            count += 1\n            \n            # Yield in batches for better efficiency\n            if len(buffer) >= self.prefetch_batches:\n                for item in buffer:\n                    yield item\n                buffer = []\n        \n        # Yield remaining items\n        for item in buffer:\n            yield item\n\n    def _extract_text(self, item):\n        if self.instruction_field and self.instruction_field in item:\n            if isinstance(item[self.instruction_field], list):\n                instruction = item[self.instruction_field][0] if item[self.instruction_field] else \"\"\n            else:\n                instruction = item[self.instruction_field]\n\n            if self.text_field in item:\n                if isinstance(item[self.text_field], list):\n                    response = item[self.text_field][0] if item[self.text_field] else \"\"\n                else:\n                    response = item[self.text_field]\n                return f\"Instruction: {instruction}\\n\\nResponse: {response}\"\n            return instruction\n\n        if self.text_field in item:\n            text = item[self.text_field]\n            if isinstance(text, list):\n                return text[0] if text else \"\"\n            return text\n\n        for key in ['text', 'content', 'document', 'article', 'response', 'output']:\n            if key in item:\n                value = item[key]\n                if isinstance(value, list):\n                    return value[0] if value else \"\"\n                return value\n\n        return \"\"\n    \n    def get_cache_size(self):\n        \"\"\"Get the current cache directory size in GB.\"\"\"\n        if not os.path.exists(self.cache_dir):\n            return 0\n        \n        total_size = 0\n        for dirpath, dirnames, filenames in os.walk(self.cache_dir):\n            for filename in filenames:\n                filepath = os.path.join(dirpath, filename)\n                total_size += os.path.getsize(filepath)\n        \n        return total_size / (1024**3)  # Convert to GB\n\n\nclass MultiDatasetLoader:\n    \"\"\"Multi-dataset loader with support for Pile and other large datasets.\"\"\"\n    \n    DATASET_CONFIGS = {\n        'wikipedia': {\n            'name': 'wikipedia',\n            'config': '20231101.en',\n            'text_field': 'text',\n        },\n        'openwebtext': {\n            'name': 'openwebtext',\n            'text_field': 'text',\n        },\n        'wikitext': {\n            'name': 'wikitext',\n            'config': 'wikitext-103-v1',\n            'text_field': 'text',\n        },\n        'bookcorpus': {\n            'name': 'bookcorpus',\n            'text_field': 'text',\n        },\n        'c4': {\n            'name': 'c4',\n            'config': 'en',\n            'text_field': 'text',\n        },\n        'dolly': {\n            'name': 'databricks/databricks-dolly-15k',\n            'text_field': 'response',\n            'instruction_field': 'instruction',\n        },\n        'alpaca': {\n            'name': 'tatsu-lab/alpaca',\n            'text_field': 'output',\n            'instruction_field': 'instruction',\n        },\n        'squad': {\n            'name': 'squad',\n            'text_field': 'context',\n        },\n        'pile': {\n            'name': 'EleutherAI/pile',\n            'text_field': 'text',\n            'config': 'all',  # Can also use specific subsets\n        },\n        'pile_cc': {\n            'name': 'EleutherAI/pile',\n            'config': 'pile_cc',\n            'text_field': 'text',\n        },\n        'pile_pubmed': {\n            'name': 'EleutherAI/pile',\n            'config': 'pubmed_central',\n            'text_field': 'text',\n        },\n        'pile_arxiv': {\n            'name': 'EleutherAI/pile',\n            'config': 'arxiv',\n            'text_field': 'text',\n        },\n        'pile_books3': {\n            'name': 'EleutherAI/pile',\n            'config': 'books3',\n            'text_field': 'text',\n        },\n        'pile_openwebtext2': {\n            'name': 'EleutherAI/pile',\n            'config': 'openwebtext2',\n            'text_field': 'text',\n        },\n    }\n\n    @classmethod\n    def create_dataset(cls, dataset_key, tokenizer, max_samples=100000,\n                       max_len=512, split='train', streaming=True,\n                       cache_dir=None, use_disk_cache=True):\n        if dataset_key not in cls.DATASET_CONFIGS:\n            raise ValueError(f\"Unknown dataset: {dataset_key}. Available: {list(cls.DATASET_CONFIGS.keys())}\")\n\n        config = cls.DATASET_CONFIGS[dataset_key]\n        return HuggingFaceDataset(\n            dataset_name=config['name'],\n            tokenizer=tokenizer,\n            max_samples=max_samples,\n            max_len=max_len,\n            split=split,\n            config=config.get('config'),\n            streaming=streaming,\n            text_field=config['text_field'],\n            instruction_field=config.get('instruction_field'),\n            cache_dir=cache_dir,\n            use_disk_cache=use_disk_cache\n        )\n    \n    @classmethod\n    def create_pile_dataset(cls, tokenizer, subsets=None, max_samples_per_subset=1_000_000,\n                           max_len=512, cache_dir=None, use_disk_cache=True):\n        \"\"\"Create a dataset from multiple Pile subsets.\"\"\"\n        if subsets is None:\n            subsets = ['pile_cc', 'pile_pubmed', 'pile_arxiv', 'pile_openwebtext2']\n        \n        datasets = []\n        for subset in subsets:\n            if subset in cls.DATASET_CONFIGS:\n                try:\n                    ds = cls.create_dataset(\n                        subset, tokenizer, \n                        max_samples=max_samples_per_subset,\n                        max_len=max_len,\n                        streaming=True,\n                        cache_dir=cache_dir,\n                        use_disk_cache=use_disk_cache\n                    )\n                    datasets.append(ds)\n                    print(f\"✅ Added Pile subset: {subset}\")\n                except Exception as e:\n                    print(f\"⚠️ Failed to load {subset}: {e}\")\n        \n        return datasets\n\n    @classmethod\n    def list_available_datasets(cls):\n        return list(cls.DATASET_CONFIGS.keys())\n\n    @classmethod\n    def get_dataset_info(cls, dataset_key):\n        if dataset_key in cls.DATASET_CONFIGS:\n            return cls.DATASET_CONFIGS[dataset_key]\n        return None\n\nprint(\"✅ Enhanced dataset loader with caching defined\")\nprint(f\"📚 Available datasets: {MultiDatasetLoader.list_available_datasets()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:41.852941Z","iopub.execute_input":"2025-10-25T04:26:41.853244Z","iopub.status.idle":"2025-10-25T04:26:41.877335Z","shell.execute_reply.started":"2025-10-25T04:26:41.853225Z","shell.execute_reply":"2025-10-25T04:26:41.876573Z"}},"outputs":[{"name":"stdout","text":"✅ Enhanced dataset loader with caching defined\n📚 Available datasets: ['wikipedia', 'openwebtext', 'wikitext', 'bookcorpus', 'c4', 'dolly', 'alpaca', 'squad', 'pile', 'pile_cc', 'pile_pubmed', 'pile_arxiv', 'pile_books3', 'pile_openwebtext2']\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## 🎓 Advanced Training Engine with Gradient Accumulation\n\nSupports checkpoint resuming, gradient accumulation, and multi-GPU training","metadata":{}},{"cell_type":"code","source":"class LLMTrainer:\n    \"\"\"Advanced trainer with gradient accumulation, checkpoint resuming, and resource monitoring.\"\"\"\n    \n    def __init__(self, model_name='cyberdyne-llm', vocab_size=None, emb_size=768,\n                 n_layers=12, n_heads=12, ff_size=3072, max_len=512,\n                 learning_rate=5e-5, weight_decay=0.01, warmup_steps=1000,\n                 device=None, use_gradient_checkpoint=False, mixed_precision=False):\n        \n        self.model_name = model_name\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n        self.mixed_precision = mixed_precision and self.device == 'cuda'\n        \n        # Initialize tokenizer\n        if vocab_size is None:\n            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n            vocab_size = len(tokenizer)\n\n        # Initialize model\n        self.model = AdvancedLLM(\n            vocab_size=vocab_size,\n            emb_size=emb_size,\n            n_layers=n_layers,\n            n_heads=n_heads,\n            ff_size=ff_size,\n            max_len=max_len,\n            use_gradient_checkpoint=use_gradient_checkpoint\n        ).to(self.device)\n        \n        # Multi-GPU support\n        self.n_gpus = torch.cuda.device_count() if self.device == 'cuda' else 0\n        if self.n_gpus > 1:\n            print(f\"🚀 Using {self.n_gpus} GPUs for training\")\n            self.model = nn.DataParallel(self.model)\n        \n        # Optimizer with weight decay\n        self.optimizer = torch.optim.AdamW(\n            self.model.parameters(), \n            lr=learning_rate,\n            weight_decay=weight_decay,\n            betas=(0.9, 0.95)  # Llama-style betas\n        )\n        \n        # Learning rate scheduler\n        self.warmup_steps = warmup_steps\n        self.scheduler = None\n        \n        # Loss function\n        self.loss_fn = nn.CrossEntropyLoss()\n        \n        # Mixed precision training\n        self.scaler = torch.amp.GradScaler('cuda') if self.mixed_precision else None\n        \n        # Training state\n        self.global_step = 0\n        self.best_loss = float('inf')\n        \n        # Get actual model for parameter counting\n        base_model = self.model.module if hasattr(self.model, 'module') else self.model\n        print(f\"🤖 Model initialized with {base_model.get_num_params():,} parameters\")\n        print(f\"🖥️  Using device: {self.device}\")\n        \n        if use_gradient_checkpoint:\n            print(\"💾 Gradient checkpointing enabled for memory efficiency\")\n        if self.mixed_precision:\n            print(\"⚡ Mixed precision training enabled\")\n\n    def train(self, dataset_key, num_epochs=3, batch_size=4, max_samples=10000,\n              max_len=512, gradient_accumulation_steps=8, \n              checkpoint_interval=1000, save_dir='models', \n              checkpoint_dir='checkpoints', log_interval=10,\n              resume_from_checkpoint=None, cache_dir=None,\n              eval_interval=None, eval_samples=1000):\n        \n        print(f\"\\n🎓 Starting training on dataset: {dataset_key}\")\n        print(f\"   Epochs: {num_epochs}, Batch size: {batch_size}\")\n        print(f\"   Gradient accumulation: {gradient_accumulation_steps}\")\n        print(f\"   Effective batch size: {batch_size * gradient_accumulation_steps * max(1, self.n_gpus)}\")\n        print(f\"   Max samples: {max_samples:,}\")\n        \n        os.makedirs(save_dir, exist_ok=True)\n        os.makedirs(checkpoint_dir, exist_ok=True)\n        \n        # Load checkpoint if resuming\n        start_epoch = 0\n        if resume_from_checkpoint and os.path.exists(resume_from_checkpoint):\n            start_epoch, self.global_step = self._load_checkpoint(resume_from_checkpoint)\n            print(f\"📂 Resumed from checkpoint: epoch {start_epoch}, step {self.global_step}\")\n        \n        # Initialize tokenizer\n        tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        \n        # Create dataset with caching\n        try:\n            dataset = MultiDatasetLoader.create_dataset(\n                dataset_key,\n                tokenizer,\n                max_samples=max_samples,\n                max_len=max_len,\n                streaming=True,\n                cache_dir=cache_dir or KAGGLE_CACHE_DIR,\n                use_disk_cache=True\n            )\n        except Exception as e:\n            print(f\"❌ Error loading dataset: {e}\")\n            return None\n        \n        # Create dataloader\n        dataloader = DataLoader(\n            dataset, \n            batch_size=batch_size, \n            num_workers=0,\n            pin_memory=True if self.device == 'cuda' else False\n        )\n        \n        # Setup learning rate scheduler\n        total_steps = (max_samples // (batch_size * gradient_accumulation_steps)) * num_epochs\n        self.scheduler = self._get_linear_schedule_with_warmup(total_steps)\n        \n        # Training history\n        training_start = time.time()\n        training_history = {\n            'model_name': self.model_name,\n            'dataset': dataset_key,\n            'epochs': num_epochs,\n            'batch_size': batch_size,\n            'gradient_accumulation_steps': gradient_accumulation_steps,\n            'losses': [],\n            'epoch_losses': [],\n            'checkpoints': []\n        }\n        \n        self.model.train()\n        accumulated_loss = 0\n        \n        for epoch in range(start_epoch, num_epochs):\n            epoch_loss = 0.0\n            batch_count = 0\n            \n            progress_bar = tqdm(\n                dataloader, \n                desc=f\"Epoch {epoch+1}/{num_epochs}\",\n                total=max_samples // batch_size\n            )\n            \n            for batch_idx, (inputs, targets) in enumerate(progress_bar):\n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n                \n                # Mixed precision training\n                if self.mixed_precision:\n                    with torch.cuda.amp.autocast():\n                        logits = self.model(inputs)\n                        loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n                        loss = loss / gradient_accumulation_steps\n                    \n                    self.scaler.scale(loss).backward()\n                else:\n                    logits = self.model(inputs)\n                    loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n                    loss = loss / gradient_accumulation_steps\n                    loss.backward()\n                \n                accumulated_loss += loss.item()\n                \n                # Gradient accumulation\n                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n                    if self.mixed_precision:\n                        self.scaler.unscale_(self.optimizer)\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                        self.scaler.step(self.optimizer)\n                        self.scaler.update()\n                    else:\n                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n                        self.optimizer.step()\n                    \n                    if self.scheduler:\n                        self.scheduler.step()\n                    \n                    self.optimizer.zero_grad()\n                    self.global_step += 1\n                    \n                    # Update metrics\n                    current_loss = accumulated_loss * gradient_accumulation_steps\n                    epoch_loss += current_loss\n                    batch_count += 1\n                    accumulated_loss = 0\n                    \n                    # Update progress bar\n                    current_lr = self.optimizer.param_groups[0]['lr']\n                    progress_bar.set_postfix({\n                        'loss': f'{current_loss:.4f}',\n                        'lr': f'{current_lr:.2e}',\n                        'step': self.global_step\n                    })\n                    \n                    # Logging\n                    if self.global_step % log_interval == 0:\n                        training_history['losses'].append({\n                            'epoch': epoch + 1,\n                            'step': self.global_step,\n                            'loss': current_loss,\n                            'lr': current_lr\n                        })\n                    \n                    # Save checkpoint\n                    if self.global_step % checkpoint_interval == 0:\n                        checkpoint_path = self._save_checkpoint(\n                            checkpoint_dir, epoch, batch_idx\n                        )\n                        training_history['checkpoints'].append(checkpoint_path)\n                        \n                        # Monitor resources\n                        if self.global_step % (checkpoint_interval * 2) == 0:\n                            monitor_resources()\n                            # Clear cache if needed\n                            if self.device == 'cuda':\n                                torch.cuda.empty_cache()\n                            gc.collect()\n                \n                # Early stopping if we've processed enough samples\n                if (batch_idx + 1) * batch_size >= max_samples:\n                    break\n            \n            # Epoch statistics\n            avg_epoch_loss = epoch_loss / max(batch_count, 1)\n            training_history['epoch_losses'].append(avg_epoch_loss)\n            print(f\"✅ Epoch {epoch+1} completed. Average loss: {avg_epoch_loss:.4f}\")\n            \n            # Save epoch checkpoint\n            epoch_checkpoint = os.path.join(\n                save_dir, f\"{self.model_name}_epoch_{epoch+1}.pt\"\n            )\n            self._save_model(epoch_checkpoint)\n            \n            # Evaluation\n            if eval_interval and (epoch + 1) % eval_interval == 0:\n                self._evaluate(dataset_key, tokenizer, eval_samples, max_len)\n        \n        # Training completion\n        training_duration = time.time() - training_start\n        training_history['duration'] = training_duration\n        \n        # Save final model\n        final_path = os.path.join(save_dir, f\"{self.model_name}_final.pt\")\n        self._save_model(final_path)\n        \n        # Save training history\n        history_path = os.path.join(save_dir, f\"{self.model_name}_history.json\")\n        with open(history_path, 'w') as f:\n            json.dump(training_history, f, indent=2)\n        \n        print(f\"\\n🎉 Training completed in {training_duration/60:.2f} minutes\")\n        print(f\"💾 Final model saved to: {final_path}\")\n        print(f\"📊 Training history saved to: {history_path}\")\n        \n        # Final resource check\n        monitor_resources()\n        \n        return training_history\n    \n    def _save_model(self, path):\n        \"\"\"Save model with proper handling for DataParallel.\"\"\"\n        model_to_save = self.model.module if hasattr(self.model, 'module') else self.model\n        model_to_save.save_model(path)\n    \n    def _save_checkpoint(self, checkpoint_dir, epoch, batch_idx):\n        \"\"\"Save training checkpoint.\"\"\"\n        checkpoint_path = os.path.join(\n            checkpoint_dir, \n            f\"checkpoint_epoch_{epoch+1}_step_{self.global_step}.pt\"\n        )\n        \n        model_state = self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict()\n        \n        checkpoint = {\n            'epoch': epoch,\n            'batch_idx': batch_idx,\n            'global_step': self.global_step,\n            'model_state_dict': model_state,\n            'optimizer_state_dict': self.optimizer.state_dict(),\n            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n            'best_loss': self.best_loss,\n            'model_config': {\n                'vocab_size': self.model.module.vocab_size if hasattr(self.model, 'module') else self.model.vocab_size,\n                'emb_size': self.model.module.emb_size if hasattr(self.model, 'module') else self.model.emb_size,\n                'n_layers': self.model.module.n_layers if hasattr(self.model, 'module') else self.model.n_layers,\n                'n_heads': self.model.module.n_heads if hasattr(self.model, 'module') else self.model.n_heads,\n                'max_len': self.model.module.max_len if hasattr(self.model, 'module') else self.model.max_len,\n            }\n        }\n        \n        torch.save(checkpoint, checkpoint_path)\n        print(f\"💾 Checkpoint saved: {checkpoint_path}\")\n        \n        # Clean old checkpoints to save space (keep only last 3)\n        self._cleanup_old_checkpoints(checkpoint_dir, keep_last=3)\n        \n        return checkpoint_path\n    \n    def _load_checkpoint(self, checkpoint_path):\n        \"\"\"Load training checkpoint.\"\"\"\n        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n        \n        if hasattr(self.model, 'module'):\n            self.model.module.load_state_dict(checkpoint['model_state_dict'])\n        else:\n            self.model.load_state_dict(checkpoint['model_state_dict'])\n        \n        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        \n        if checkpoint.get('scheduler_state_dict') and self.scheduler:\n            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n        \n        self.global_step = checkpoint['global_step']\n        self.best_loss = checkpoint.get('best_loss', float('inf'))\n        \n        return checkpoint['epoch'], checkpoint['global_step']\n    \n    def _cleanup_old_checkpoints(self, checkpoint_dir, keep_last=3):\n        \"\"\"Remove old checkpoints to save disk space.\"\"\"\n        checkpoints = sorted(\n            [f for f in os.listdir(checkpoint_dir) if f.startswith('checkpoint_')],\n            key=lambda x: os.path.getmtime(os.path.join(checkpoint_dir, x))\n        )\n        \n        if len(checkpoints) > keep_last:\n            for checkpoint in checkpoints[:-keep_last]:\n                os.remove(os.path.join(checkpoint_dir, checkpoint))\n                print(f\"🗑️ Removed old checkpoint: {checkpoint}\")\n    \n    def _get_linear_schedule_with_warmup(self, total_steps):\n        \"\"\"Create linear learning rate schedule with warmup.\"\"\"\n        def lr_lambda(step):\n            if step < self.warmup_steps:\n                return float(step) / float(max(1, self.warmup_steps))\n            return max(\n                0.0, float(total_steps - step) / float(max(1, total_steps - self.warmup_steps))\n            )\n        \n        return torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n    \n    def _evaluate(self, dataset_key, tokenizer, eval_samples=1000, max_len=512):\n        \"\"\"Evaluate model during training.\"\"\"\n        print(f\"\\n📊 Evaluating on {eval_samples} samples...\")\n        self.model.eval()\n        \n        # Create evaluation dataset\n        eval_dataset = MultiDatasetLoader.create_dataset(\n            dataset_key,\n            tokenizer,\n            max_samples=eval_samples,\n            max_len=max_len,\n            split='validation' if 'validation' in dataset_key else 'train',\n            streaming=True\n        )\n        \n        eval_loader = DataLoader(eval_dataset, batch_size=8, num_workers=0)\n        \n        total_loss = 0\n        batch_count = 0\n        \n        with torch.no_grad():\n            for inputs, targets in tqdm(eval_loader, desc=\"Evaluating\", total=eval_samples//8):\n                inputs = inputs.to(self.device)\n                targets = targets.to(self.device)\n                \n                logits = self.model(inputs)\n                loss = self.loss_fn(logits.view(-1, logits.size(-1)), targets.view(-1))\n                \n                total_loss += loss.item()\n                batch_count += 1\n                \n                if batch_count * 8 >= eval_samples:\n                    break\n        \n        avg_loss = total_loss / max(batch_count, 1)\n        print(f\"📈 Evaluation loss: {avg_loss:.4f}\")\n        \n        self.model.train()\n        return avg_loss\n\nprint(\"✅ Advanced training engine with gradient accumulation defined\")\nprint(\"   Features: Gradient accumulation, checkpoint resuming, mixed precision, multi-GPU support\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:41.877956Z","iopub.execute_input":"2025-10-25T04:26:41.878205Z","iopub.status.idle":"2025-10-25T04:26:42.029600Z","shell.execute_reply.started":"2025-10-25T04:26:41.878187Z","shell.execute_reply":"2025-10-25T04:26:42.028764Z"}},"outputs":[{"name":"stdout","text":"✅ Advanced training engine with gradient accumulation defined\n   Features: Gradient accumulation, checkpoint resuming, mixed precision, multi-GPU support\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## 💬 Inference Engine\n\nConversational AI with context management","metadata":{}},{"cell_type":"code","source":"class ConversationalInference:\n    def __init__(self, model, tokenizer_name='gpt2', device=None, max_context_length=5):\n        self.model = model\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        self.model.eval()\n\n        self.tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_name)\n        self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n        self.max_context_length = max_context_length\n        self.conversations = {}\n\n        print(f\"💬 Inference engine initialized on {self.device}\")\n\n    def create_session(self):\n        session_id = str(uuid.uuid4())\n        self.conversations[session_id] = []\n        return session_id\n\n    def get_conversation_history(self, session_id):\n        return self.conversations.get(session_id, [])\n\n    def clear_session(self, session_id):\n        if session_id in self.conversations:\n            self.conversations[session_id] = []\n\n    def generate_response(self, prompt, session_id=None, max_new_tokens=150,\n                         temperature=0.8, top_k=50, top_p=0.95,\n                         use_context=True):\n        if session_id is None:\n            session_id = self.create_session()\n\n        if session_id not in self.conversations:\n            self.conversations[session_id] = []\n\n        if use_context and self.conversations[session_id]:\n            context_messages = self.conversations[session_id][-self.max_context_length:]\n            context_text = self._build_context(context_messages)\n            full_prompt = f\"{context_text}\\nUser: {prompt}\\nAssistant:\"\n        else:\n            full_prompt = f\"User: {prompt}\\nAssistant:\"\n\n        response = self._generate_text(\n            full_prompt,\n            max_new_tokens=max_new_tokens,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p\n        )\n\n        self.conversations[session_id].append({\n            'role': 'user',\n            'content': prompt\n        })\n        self.conversations[session_id].append({\n            'role': 'assistant',\n            'content': response\n        })\n\n        return response, session_id\n\n    def _build_context(self, messages):\n        context_parts = []\n        for msg in messages:\n            if msg['role'] == 'user':\n                context_parts.append(f\"User: {msg['content']}\")\n            else:\n                context_parts.append(f\"Assistant: {msg['content']}\")\n        return \"\\n\".join(context_parts)\n\n    def _generate_text(self, prompt, max_new_tokens=150, temperature=0.8,\n                       top_k=50, top_p=0.95):\n        tokens = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n\n        if tokens.size(1) > self.model.max_len - max_new_tokens:\n            tokens = tokens[:, -(self.model.max_len - max_new_tokens):]\n\n        generated = tokens\n\n        with torch.no_grad():\n            for _ in range(max_new_tokens):\n                if generated.size(1) >= self.model.max_len:\n                    break\n\n                logits = self.model(generated)\n                next_token_logits = logits[:, -1, :]\n\n                next_token_logits = next_token_logits / temperature\n\n                sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)\n                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n\n                sorted_indices_to_remove = cumulative_probs > top_p\n                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n                sorted_indices_to_remove[..., 0] = 0\n\n                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n                next_token_logits[indices_to_remove] = float('-inf')\n\n                top_probs, top_indices = torch.topk(torch.softmax(next_token_logits, dim=-1), top_k)\n                next_token = top_indices[0, torch.multinomial(top_probs[0], 1)]\n\n                generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)\n\n\n                if next_token.item() == self.tokenizer.eos_token_id:\n                    break\n\n        output_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n\n        if \"Assistant:\" in output_text:\n            response = output_text.split(\"Assistant:\")[-1].strip()\n        else:\n            response = output_text[len(prompt):].strip()\n\n        return response\n\n    def chat(self, message, session_id=None, **kwargs):\n        return self.generate_response(message, session_id=session_id, **kwargs)\n\n\nclass OfflineInference:\n    def __init__(self, model_path, device=None):\n        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n\n        print(f\"📂 Loading model from: {model_path}\")\n        checkpoint = torch.load(model_path, map_location=self.device)\n\n        self.model = AdvancedLLM(\n            vocab_size=checkpoint['vocab_size'],\n            emb_size=checkpoint['emb_size'],\n            n_layers=checkpoint['n_layers'],\n            n_heads=checkpoint['n_heads'],\n            max_len=checkpoint['max_len'],\n            use_gradient_checkpoint=checkpoint.get('use_gradient_checkpoint', False)\n        )\n        self.model.load_state_dict(checkpoint['state_dict'])\n        self.model.to(self.device)\n\n        tokenizer_name = checkpoint.get('tokenizer_name', 'gpt2')\n        self.inference_engine = ConversationalInference(\n            self.model,\n            tokenizer_name=tokenizer_name,\n            device=self.device\n        )\n\n        print(\"✅ Offline inference ready!\")\n\n    def chat(self, message, session_id=None, **kwargs):\n        return self.inference_engine.chat(message, session_id=session_id, **kwargs)\n\n    def create_session(self):\n        return self.inference_engine.create_session()\n\n    def clear_session(self, session_id):\n        self.inference_engine.clear_session(session_id)\n\n    def get_history(self, session_id):\n        return self.inference_engine.get_conversation_history(session_id)\n\nprint(\"✅ Inference engine defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:42.030493Z","iopub.execute_input":"2025-10-25T04:26:42.030805Z","iopub.status.idle":"2025-10-25T04:26:42.051751Z","shell.execute_reply.started":"2025-10-25T04:26:42.030777Z","shell.execute_reply":"2025-10-25T04:26:42.050929Z"}},"outputs":[{"name":"stdout","text":"✅ Inference engine defined\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 🚀 Quick Training (1M Samples)\n\nOptimized for Kaggle's 50GB SSD - Quick training with 1 million samples","metadata":{}},{"cell_type":"code","source":"# Quick training configuration - 1M samples\nprint(\"🚀 Quick Training Configuration\")\nprint(\"=\"*60)\n\n# Monitor resources before training\ndisk_info, mem_info = monitor_resources()\n\n# Select configuration\nconfig = TRAINING_CONFIGS['quick']\nprint(f\"\\n📋 Using configuration: {config['description']}\")\nprint(f\"   Max samples: {config['max_samples']:,}\")\nprint(f\"   Batch size: {config['batch_size']}\")\nprint(f\"   Gradient accumulation: {config['gradient_accumulation_steps']}\")\nprint(f\"   Effective batch size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n\n# Initialize trainer with memory optimization\nquick_trainer = LLMTrainer(\n    model_name='cyberdyne-quick-1M',\n    emb_size=512,\n    n_layers=6,\n    n_heads=8,\n    learning_rate=5e-5,\n    warmup_steps=500,\n    use_gradient_checkpoint=True,  # Enable gradient checkpointing\n    mixed_precision=True  # Enable mixed precision\n)\n\n# Estimate memory usage\n# Handle DataParallel wrapper if present\nmodel_for_estimate = quick_trainer.model.module if hasattr(quick_trainer.model, 'module') else quick_trainer.model\nmemory_estimate = model_for_estimate.estimate_memory_usage(\n    batch_size=config['batch_size'],\n    seq_length=512\n)\nprint(f\"\\n💾 Estimated memory usage:\")\nprint(f\"   Model parameters: {memory_estimate['params_gb']:.2f}GB\")\nprint(f\"   Activations: {memory_estimate['activations_gb']:.2f}GB\")\nprint(f\"   Total: {memory_estimate['total_gb']:.2f}GB\")\n\n# Train on wikitext dataset with 1M samples\nprint(\"\\n🎓 Starting quick training on wikitext dataset...\")\nhistory = quick_trainer.train(\n    dataset_key='wikitext',\n    num_epochs=2,\n    batch_size=config['batch_size'],\n    gradient_accumulation_steps=config['gradient_accumulation_steps'],\n    max_samples=config['max_samples'],\n    checkpoint_interval=config['checkpoint_interval'],\n    save_dir=KAGGLE_MODEL_DIR,\n    checkpoint_dir=KAGGLE_CHECKPOINT_DIR,\n    cache_dir=KAGGLE_CACHE_DIR,\n    log_interval=50,\n    eval_interval=1  # Evaluate after each epoch\n)\n\nif history:\n    print(\"\\n\" + \"=\"*60)\n    print(\"📊 Quick Training Summary\")\n    print(\"=\"*60)\n    print(f\"Model: {history['model_name']}\")\n    print(f\"Dataset: {history['dataset']}\")\n    print(f\"Epochs: {history['epochs']}\")\n    print(f\"Samples processed: {config['max_samples']:,}\")\n    print(f\"Final Loss: {history['epoch_losses'][-1]:.4f}\")\n    print(f\"Duration: {history['duration']/60:.2f} minutes\")\n    print(f\"Training speed: {config['max_samples'] / (history['duration']/60):.0f} samples/minute\")\n    print(\"=\"*60)\n    \n    # Check disk usage after training\n    print(\"\\n📊 Post-training resource check:\")\n    monitor_resources()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T04:26:42.052449Z","iopub.execute_input":"2025-10-25T04:26:42.052656Z","iopub.status.idle":"2025-10-25T09:30:51.408363Z","shell.execute_reply.started":"2025-10-25T04:26:42.052639Z","shell.execute_reply":"2025-10-25T09:30:51.407535Z"}},"outputs":[{"name":"stdout","text":"🚀 Quick Training Configuration\n============================================================\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 19.5GB free of 19.5GB (0.0% used)\n🧠 RAM: 29.7GB free of 31.4GB (5.4% used)\n🎮 GPU: 0.0GB used, 0.0GB cached of 15.8GB\n============================================================\n\n\n📋 Using configuration: Quick training with 1M samples\n   Max samples: 237,560\n   Batch size: 8\n   Gradient accumulation: 4\n   Effective batch size: 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1652ee10bfc4d76aef5dafb87631353"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85b122599dc84251900b947ced8ea4a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cdadff2e2f24276b823ea9b8be0cea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7dcf705147845929e41e7171fec534f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21f1251e5dbf48b382679b1f14d8b527"}},"metadata":{}},{"name":"stdout","text":"🚀 Using 2 GPUs for training\n🤖 Model initialized with 45,009,408 parameters\n🖥️  Using device: cuda\n💾 Gradient checkpointing enabled for memory efficiency\n⚡ Mixed precision training enabled\n\n💾 Estimated memory usage:\n   Model parameters: 0.18GB\n   Activations: 0.05GB\n   Total: 0.41GB\n\n🎓 Starting quick training on wikitext dataset...\n\n🎓 Starting training on dataset: wikitext\n   Epochs: 2, Batch size: 8\n   Gradient accumulation: 4\n   Effective batch size: 64\n   Max samples: 237,560\n💾 Using cache directory: /kaggle/working/cache\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42c901406a8442e5842482389249de22"}},"metadata":{}},{"name":"stdout","text":"✅ Loaded dataset: wikitext\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:   0%|          | 0/29695 [00:00<?, ?it/s]/tmp/ipykernel_77/1962050053.py:153: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\n/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\nEpoch 1/2:   7%|▋         | 2000/29695 [15:55<5:53:28,  1.31it/s, loss=7.5793, lr=5.00e-05, step=500] ","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_500.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  13%|█▎        | 3999/29695 [31:44<3:22:41,  2.11it/s, loss=4.2031, lr=4.83e-05, step=1000]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_1000.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 18.5GB free of 19.5GB (5.2% used)\n🧠 RAM: 26.8GB free of 31.4GB (14.4% used)\n🎮 GPU: 1.0GB used, 4.0GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  20%|██        | 6000/29695 [47:33<5:00:38,  1.31it/s, loss=1.4510, lr=4.65e-05, step=1500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_1500.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  27%|██▋       | 7999/29695 [1:03:21<2:50:23,  2.12it/s, loss=3.3327, lr=4.48e-05, step=2000] ","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_2000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 18.0GB free of 19.5GB (7.7% used)\n🧠 RAM: 26.8GB free of 31.4GB (14.5% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  34%|███▎      | 10000/29695 [1:19:11<4:23:33,  1.25it/s, loss=6.3364, lr=4.30e-05, step=2500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_2500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_1000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  40%|████      | 11999/29695 [1:34:58<2:19:40,  2.11it/s, loss=4.9071, lr=4.13e-05, step=3000] ","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_3000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_1500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 18.0GB free of 19.5GB (7.7% used)\n🧠 RAM: 26.7GB free of 31.4GB (14.9% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  47%|████▋     | 14000/29695 [1:50:47<3:29:35,  1.25it/s, loss=5.0418, lr=3.95e-05, step=3500] ","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_3500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_2000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  54%|█████▍    | 15999/29695 [2:06:34<1:48:00,  2.11it/s, loss=4.0013, lr=3.78e-05, step=4000]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_4000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_2500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 18.0GB free of 19.5GB (7.7% used)\n🧠 RAM: 26.7GB free of 31.4GB (14.9% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  61%|██████    | 18000/29695 [2:22:24<2:34:42,  1.26it/s, loss=4.7974, lr=3.61e-05, step=4500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_1_step_4500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_3000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/2:  65%|██████▍   | 19197/29695 [2:31:50<1:23:02,  2.11it/s, loss=3.5425, lr=3.50e-05, step=4799]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 1 completed. Average loss: 5.4589\n✅ Model saved to /kaggle/working/models/cyberdyne-quick-1M_epoch_1.pt (0.18GB)\n\n📊 Evaluating on 1000 samples...\n💾 Using cache directory: /kaggle/working/cache\n✅ Loaded dataset: wikitext\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  65%|██████▍   | 81/125 [00:13<00:07,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"📈 Evaluation loss: 1.0564\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:   3%|▎         | 803/29695 [06:21<3:46:59,  2.12it/s, loss=2.7456, lr=3.43e-05, step=5000] ","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_5000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_3500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.8GB free of 19.5GB (8.6% used)\n🧠 RAM: 26.4GB free of 31.4GB (15.8% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:   9%|▉         | 2804/29695 [22:10<5:59:35,  1.25it/s, loss=4.3997, lr=3.26e-05, step=5500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_5500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_4000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  16%|█▌        | 4803/29695 [37:57<3:17:47,  2.10it/s, loss=7.2639, lr=3.08e-05, step=6000]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_6000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_1_step_4500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.8GB free of 19.5GB (8.6% used)\n🧠 RAM: 26.4GB free of 31.4GB (15.8% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  23%|██▎       | 6804/29695 [53:46<5:12:09,  1.22it/s, loss=4.3983, lr=2.91e-05, step=6500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_6500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_5000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  30%|██▉       | 8803/29695 [1:09:33<2:44:42,  2.11it/s, loss=3.9933, lr=2.73e-05, step=7000]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_7000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_5500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.8GB free of 19.5GB (8.6% used)\n🧠 RAM: 26.4GB free of 31.4GB (15.8% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  36%|███▋      | 10804/29695 [1:25:22<4:13:12,  1.24it/s, loss=2.5392, lr=2.56e-05, step=7500] ","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_7500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_6000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  43%|████▎     | 12803/29695 [1:41:09<2:12:34,  2.12it/s, loss=2.3715, lr=2.39e-05, step=8000]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_8000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_6500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.8GB free of 19.5GB (8.6% used)\n🧠 RAM: 26.4GB free of 31.4GB (15.9% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  50%|████▉     | 14804/29695 [1:56:58<3:24:30,  1.21it/s, loss=2.7847, lr=2.21e-05, step=8500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_8500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_7000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  57%|█████▋    | 16803/29695 [2:12:45<1:43:02,  2.09it/s, loss=6.4116, lr=2.04e-05, step=9000]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_9000.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_7500.pt\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.8GB free of 19.5GB (8.6% used)\n🧠 RAM: 26.4GB free of 31.4GB (15.8% used)\n🎮 GPU: 1.0GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  63%|██████▎   | 18804/29695 [2:28:34<2:26:02,  1.24it/s, loss=2.1818, lr=1.86e-05, step=9500]","output_type":"stream"},{"name":"stdout","text":"💾 Checkpoint saved: /kaggle/working/checkpoints/checkpoint_epoch_2_step_9500.pt\n🗑️ Removed old checkpoint: checkpoint_epoch_2_step_8000.pt\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/2:  65%|██████▍   | 19197/29695 [2:31:40<1:22:56,  2.11it/s, loss=3.3948, lr=1.83e-05, step=9598]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 2 completed. Average loss: 4.3524\n✅ Model saved to /kaggle/working/models/cyberdyne-quick-1M_epoch_2.pt (0.18GB)\n\n📊 Evaluating on 1000 samples...\n💾 Using cache directory: /kaggle/working/cache\n✅ Loaded dataset: wikitext\n","output_type":"stream"},{"name":"stderr","text":"Evaluating:  65%|██████▍   | 81/125 [00:13<00:07,  6.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"📈 Evaluation loss: 1.0057\n✅ Model saved to /kaggle/working/models/cyberdyne-quick-1M_final.pt (0.18GB)\n\n🎉 Training completed in 304.00 minutes\n💾 Final model saved to: /kaggle/working/models/cyberdyne-quick-1M_final.pt\n📊 Training history saved to: /kaggle/working/models/cyberdyne-quick-1M_history.json\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.5GB free of 19.5GB (10.3% used)\n🧠 RAM: 26.3GB free of 31.4GB (16.0% used)\n🎮 GPU: 0.9GB used, 4.4GB cached of 15.8GB\n============================================================\n\n\n============================================================\n📊 Quick Training Summary\n============================================================\nModel: cyberdyne-quick-1M\nDataset: wikitext\nEpochs: 2\nSamples processed: 237,560\nFinal Loss: 4.3524\nDuration: 304.00 minutes\nTraining speed: 781 samples/minute\n============================================================\n\n📊 Post-training resource check:\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.5GB free of 19.5GB (10.3% used)\n🧠 RAM: 26.3GB free of 31.4GB (16.0% used)\n🎮 GPU: 0.7GB used, 4.4GB cached of 15.8GB\n============================================================\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 💪 Standard Training (5M Samples)\n\nMedium-scale training with 5 million samples","metadata":{}},{"cell_type":"code","source":"# Standard training configuration - 5M samples\nprint(\"💪 Standard Training Configuration\")\nprint(\"=\"*60)\n\n# Monitor resources before training\ndisk_info, mem_info = monitor_resources()\n\n# Select configuration\nconfig = TRAINING_CONFIGS['standard']\nprint(f\"\\n📋 Using configuration: {config['description']}\")\nprint(f\"   Max samples: {config['max_samples']:,}\")\nprint(f\"   Batch size: {config['batch_size']}\")\nprint(f\"   Gradient accumulation: {config['gradient_accumulation_steps']}\")\n\n# Initialize trainer\nstandard_trainer = LLMTrainer(\n    model_name='cyberdyne-standard-5M',\n    emb_size=768,\n    n_layers=8,\n    n_heads=12,\n    ff_size=3072,\n    learning_rate=3e-5,\n    warmup_steps=1000,\n    use_gradient_checkpoint=True,\n    mixed_precision=True\n)\n\n# Train on dolly instruction dataset\nprint(\"\\n🎓 Starting standard training on dolly dataset...\")\nhistory = standard_trainer.train(\n    dataset_key='dolly',\n    num_epochs=3,\n    batch_size=config['batch_size'],\n    gradient_accumulation_steps=config['gradient_accumulation_steps'],\n    max_samples=config['max_samples'],\n    checkpoint_interval=config['checkpoint_interval'],\n    save_dir=KAGGLE_MODEL_DIR,\n    checkpoint_dir=KAGGLE_CHECKPOINT_DIR,\n    cache_dir=KAGGLE_CACHE_DIR,\n    log_interval=100,\n    resume_from_checkpoint=None  # Set to checkpoint path to resume\n)\n\nif history:\n    print(\"\\n\" + \"=\"*60)\n    print(\"📊 Standard Training Summary\")\n    print(\"=\"*60)\n    print(f\"Model: {history['model_name']}\")\n    print(f\"Dataset: {history['dataset']}\")\n    print(f\"Duration: {history['duration']/3600:.2f} hours\")\n    print(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T09:30:51.409202Z","iopub.execute_input":"2025-10-25T09:30:51.409492Z","iopub.status.idle":"2025-10-25T11:10:43.647000Z","shell.execute_reply.started":"2025-10-25T09:30:51.409465Z","shell.execute_reply":"2025-10-25T11:10:43.646318Z"}},"outputs":[{"name":"stdout","text":"💪 Standard Training Configuration\n============================================================\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 17.5GB free of 19.5GB (10.3% used)\n🧠 RAM: 26.3GB free of 31.4GB (16.0% used)\n🎮 GPU: 0.7GB used, 4.4GB cached of 15.8GB\n============================================================\n\n\n📋 Using configuration: Standard training with 5M samples\n   Max samples: 1,187,804\n   Batch size: 8\n   Gradient accumulation: 8\n🚀 Using 2 GPUs for training\n🤖 Model initialized with 95,240,448 parameters\n🖥️  Using device: cuda\n💾 Gradient checkpointing enabled for memory efficiency\n⚡ Mixed precision training enabled\n\n🎓 Starting standard training on dolly dataset...\n\n🎓 Starting training on dataset: dolly\n   Epochs: 3, Batch size: 8\n   Gradient accumulation: 8\n   Effective batch size: 128\n   Max samples: 1,187,804\n💾 Using cache directory: /kaggle/working/cache\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c3c593eb5f348dfbbbacfee6ba5d488"}},"metadata":{}},{"name":"stdout","text":"✅ Loaded dataset: databricks/databricks-dolly-15k\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/3:   0%|          | 0/148475 [00:00<?, ?it/s]/tmp/ipykernel_77/1962050053.py:153: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():\nEpoch 1/3:   1%|▏         | 1877/148475 [33:17<43:19:46,  1.06s/it, loss=14.4490, lr=7.02e-06, step=234]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 1 completed. Average loss: 31.4455\n✅ Model saved to /kaggle/working/models/cyberdyne-standard-5M_epoch_1.pt (0.38GB)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/3:   1%|▏         | 1877/148475 [33:14<43:16:30,  1.06s/it, loss=11.4922, lr=1.40e-05, step=468]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 2 completed. Average loss: 12.5462\n✅ Model saved to /kaggle/working/models/cyberdyne-standard-5M_epoch_2.pt (0.38GB)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/3:   1%|▏         | 1877/148475 [33:14<43:15:44,  1.06s/it, loss=10.3081, lr=2.11e-05, step=702]\n","output_type":"stream"},{"name":"stdout","text":"✅ Epoch 3 completed. Average loss: 10.4863\n✅ Model saved to /kaggle/working/models/cyberdyne-standard-5M_epoch_3.pt (0.38GB)\n✅ Model saved to /kaggle/working/models/cyberdyne-standard-5M_final.pt (0.38GB)\n\n🎉 Training completed in 99.80 minutes\n💾 Final model saved to: /kaggle/working/models/cyberdyne-standard-5M_final.pt\n📊 Training history saved to: /kaggle/working/models/cyberdyne-standard-5M_history.json\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 16.1GB free of 19.5GB (17.6% used)\n🧠 RAM: 26.2GB free of 31.4GB (16.4% used)\n🎮 GPU: 2.4GB used, 6.0GB cached of 15.8GB\n============================================================\n\n\n============================================================\n📊 Standard Training Summary\n============================================================\nModel: cyberdyne-standard-5M\nDataset: dolly\nDuration: 1.66 hours\n============================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## 🔥 Large-Scale Pile Training (20M+ Samples)\n\nMaximum scale training using the Pile dataset","metadata":{}},{"cell_type":"code","source":"# Large-scale Pile training - Maximum samples that fit in 50GB\nprint(\"🔥 Large-Scale Pile Training Configuration\")\nprint(\"=\"*60)\n\n# Monitor resources\ndisk_info, mem_info = monitor_resources()\n\n# Select configuration\nconfig = TRAINING_CONFIGS['pile']\nprint(f\"\\n📋 Using configuration: {config['description']}\")\nprint(f\"   Max samples: {config['max_samples']:,}\")\nprint(f\"   This will use most of the available {DATA_CACHE_SIZE_GB:.1f}GB cache space\")\n\n# Initialize large model with maximum memory optimization\npile_trainer = LLMTrainer(\n    model_name='cyberdyne-pile-large',\n    emb_size=768,\n    n_layers=12,\n    n_heads=12,\n    ff_size=3072,\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_steps=2000,\n    use_gradient_checkpoint=True,  # Essential for large models\n    mixed_precision=True  # Essential for speed and memory\n)\n\n# Pile dataset configuration\nPILE_SUBSETS = [\n    'pile_cc',           # Common Crawl\n    'pile_pubmed',       # PubMed Central\n    'pile_arxiv',        # ArXiv\n    'pile_openwebtext2', # OpenWebText2\n]\n\nprint(f\"\\n📚 Will train on Pile subsets: {', '.join(PILE_SUBSETS)}\")\nprint(f\"   Samples per subset: {config['max_samples'] // len(PILE_SUBSETS):,}\")\n\n# Create tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Option 1: Train on a single Pile subset\nprint(\"\\n🎓 Starting large-scale training on Pile dataset...\")\nhistory = pile_trainer.train(\n    dataset_key='pile_cc',  # Start with Common Crawl subset\n    num_epochs=2,\n    batch_size=config['batch_size'],\n    gradient_accumulation_steps=config['gradient_accumulation_steps'],\n    max_samples=config['max_samples'],\n    checkpoint_interval=config['checkpoint_interval'],\n    save_dir=KAGGLE_MODEL_DIR,\n    checkpoint_dir=KAGGLE_CHECKPOINT_DIR,\n    cache_dir=KAGGLE_CACHE_DIR,\n    log_interval=200,\n    resume_from_checkpoint=None  # Can resume from checkpoint if interrupted\n)\n\n# Option 2: Train on multiple Pile subsets (uncomment to use)\n# datasets = MultiDatasetLoader.create_pile_dataset(\n#     tokenizer=tokenizer,\n#     subsets=PILE_SUBSETS,\n#     max_samples_per_subset=config['max_samples'] // len(PILE_SUBSETS),\n#     max_len=512,\n#     cache_dir=KAGGLE_CACHE_DIR,\n#     use_disk_cache=True\n# )\n\nif history:\n    print(\"\\n\" + \"=\"*60)\n    print(\"📊 Large-Scale Pile Training Summary\")\n    print(\"=\"*60)\n    print(f\"Model: {history['model_name']}\")\n    print(f\"Dataset: Pile\")\n    print(f\"Samples processed: {config['max_samples']:,}\")\n    print(f\"Duration: {history['duration']/3600:.2f} hours\")\n    print(f\"Final Loss: {history['epoch_losses'][-1]:.4f}\")\n    print(f\"Checkpoints saved: {len(history['checkpoints'])}\")\n    print(\"=\"*60)\n    \n    # Final resource check\n    print(\"\\n📊 Final resource status:\")\n    disk_info, mem_info = monitor_resources()\n    \n    # Cache usage\n    cache_size = sum(\n        os.path.getsize(os.path.join(dirpath, filename))\n        for dirpath, dirnames, filenames in os.walk(KAGGLE_CACHE_DIR)\n        for filename in filenames\n    ) / (1024**3)\n    print(f\"📦 Cache size: {cache_size:.2f}GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:10:43.647775Z","iopub.execute_input":"2025-10-25T11:10:43.648051Z","iopub.status.idle":"2025-10-25T11:10:47.418694Z","shell.execute_reply.started":"2025-10-25T11:10:43.648032Z","shell.execute_reply":"2025-10-25T11:10:47.418076Z"}},"outputs":[{"name":"stdout","text":"🔥 Large-Scale Pile Training Configuration\n============================================================\n\n============================================================\n📊 Resource Monitor\n============================================================\n💾 Disk: 16.1GB free of 19.5GB (17.6% used)\n🧠 RAM: 26.2GB free of 31.4GB (16.4% used)\n🎮 GPU: 2.3GB used, 6.0GB cached of 15.8GB\n============================================================\n\n\n📋 Using configuration: Pile dataset training with 4,751,216 samples\n   Max samples: 4,751,216\n   This will use most of the available 9.5GB cache space\n🚀 Using 2 GPUs for training\n🤖 Model initialized with 123,561,216 parameters\n🖥️  Using device: cuda\n💾 Gradient checkpointing enabled for memory efficiency\n⚡ Mixed precision training enabled\n\n📚 Will train on Pile subsets: pile_cc, pile_pubmed, pile_arxiv, pile_openwebtext2\n   Samples per subset: 1,187,804\n\n🎓 Starting large-scale training on Pile dataset...\n\n🎓 Starting training on dataset: pile_cc\n   Epochs: 2, Batch size: 4\n   Gradient accumulation: 32\n   Effective batch size: 256\n   Max samples: 4,751,216\n💾 Using cache directory: /kaggle/working/cache\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2b240be16d1456c9e1f02b3f18ab78a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pile.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c21e45b1a6e48578f48f084505a0645"}},"metadata":{}},{"name":"stdout","text":"❌ Error loading dataset EleutherAI/pile: Dataset scripts are no longer supported, but found pile.py\n❌ Error loading dataset: Dataset scripts are no longer supported, but found pile.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## 🔄 Resume Training from Checkpoint\n\nContinue interrupted training sessions","metadata":{}},{"cell_type":"code","source":"# Resume training from checkpoint\nprint(\"🔄 Resume Training from Checkpoint\")\nprint(\"=\"*60)\n\n# List available checkpoints\nif os.path.exists(KAGGLE_CHECKPOINT_DIR):\n    checkpoints = sorted([\n        f for f in os.listdir(KAGGLE_CHECKPOINT_DIR) \n        if f.endswith('.pt')\n    ])\n    \n    if checkpoints:\n        print(\"\\n📁 Available checkpoints:\")\n        for i, checkpoint in enumerate(checkpoints):\n            checkpoint_path = os.path.join(KAGGLE_CHECKPOINT_DIR, checkpoint)\n            size_gb = os.path.getsize(checkpoint_path) / (1024**3)\n            print(f\"  {i+1}. {checkpoint} ({size_gb:.2f}GB)\")\n        \n        # Use the latest checkpoint\n        latest_checkpoint = os.path.join(KAGGLE_CHECKPOINT_DIR, checkpoints[-1])\n        print(f\"\\n✅ Will resume from: {latest_checkpoint}\")\n        \n        # Resume training\n        resume_trainer = LLMTrainer(\n            model_name='cyberdyne-resumed',\n            emb_size=768,\n            n_layers=12,\n            n_heads=12,\n            learning_rate=2e-5,\n            use_gradient_checkpoint=True,\n            mixed_precision=True\n        )\n        \n        # Continue training\n        config = TRAINING_CONFIGS['standard']\n        history = resume_trainer.train(\n            dataset_key='pile_cc',\n            num_epochs=3,  # Total epochs (will continue from checkpoint)\n            batch_size=config['batch_size'],\n            gradient_accumulation_steps=config['gradient_accumulation_steps'],\n            max_samples=config['max_samples'],\n            checkpoint_interval=config['checkpoint_interval'],\n            save_dir=KAGGLE_MODEL_DIR,\n            checkpoint_dir=KAGGLE_CHECKPOINT_DIR,\n            cache_dir=KAGGLE_CACHE_DIR,\n            resume_from_checkpoint=latest_checkpoint\n        )\n    else:\n        print(\"❌ No checkpoints found. Train a model first.\")\nelse:\n    print(\"❌ Checkpoint directory doesn't exist. Train a model first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:10:47.419524Z","iopub.execute_input":"2025-10-25T11:10:47.419732Z","iopub.status.idle":"2025-10-25T11:10:50.954148Z","shell.execute_reply.started":"2025-10-25T11:10:47.419714Z","shell.execute_reply":"2025-10-25T11:10:50.952646Z"}},"outputs":[{"name":"stdout","text":"🔄 Resume Training from Checkpoint\n============================================================\n\n📁 Available checkpoints:\n  1. checkpoint_epoch_2_step_8500.pt (0.50GB)\n  2. checkpoint_epoch_2_step_9000.pt (0.50GB)\n  3. checkpoint_epoch_2_step_9500.pt (0.50GB)\n\n✅ Will resume from: /kaggle/working/checkpoints/checkpoint_epoch_2_step_9500.pt\n🚀 Using 2 GPUs for training\n🤖 Model initialized with 123,561,216 parameters\n🖥️  Using device: cuda\n💾 Gradient checkpointing enabled for memory efficiency\n⚡ Mixed precision training enabled\n\n🎓 Starting training on dataset: pile_cc\n   Epochs: 3, Batch size: 8\n   Gradient accumulation: 8\n   Effective batch size: 128\n   Max samples: 1,187,804\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_77/622856174.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# Continue training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTRAINING_CONFIGS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'standard'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         history = resume_trainer.train(\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mdataset_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pile_cc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Total epochs (will continue from checkpoint)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_77/1962050053.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, dataset_key, num_epochs, batch_size, max_samples, max_len, gradient_accumulation_steps, checkpoint_interval, save_dir, checkpoint_dir, log_interval, resume_from_checkpoint, cache_dir, eval_interval, eval_samples)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mstart_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"📂 Resumed from checkpoint: epoch {start_epoch}, step {self.global_step}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_77/1962050053.py\u001b[0m in \u001b[0;36m_load_checkpoint\u001b[0;34m(self, checkpoint_path)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'module'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2581\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   2582\u001b[0m                 \"Error(s) in loading state_dict for {}:\\n\\t{}\".format(\n\u001b[1;32m   2583\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for AdvancedLLM:\n\tMissing key(s) in state_dict: \"blocks.6.attn.qkv.weight\", \"blocks.6.attn.out.weight\", \"blocks.6.attn.out.bias\", \"blocks.6.ff.w1.weight\", \"blocks.6.ff.w2.weight\", \"blocks.6.ff.w3.weight\", \"blocks.6.ln1.weight\", \"blocks.6.ln2.weight\", \"blocks.7.attn.qkv.weight\", \"blocks.7.attn.out.weight\", \"blocks.7.attn.out.bias\", \"blocks.7.ff.w1.weight\", \"blocks.7.ff.w2.weight\", \"blocks.7.ff.w3.weight\", \"blocks.7.ln1.weight\", \"blocks.7.ln2.weight\", \"blocks.8.attn.qkv.weight\", \"blocks.8.attn.out.weight\", \"blocks.8.attn.out.bias\", \"blocks.8.ff.w1.weight\", \"blocks.8.ff.w2.weight\", \"blocks.8.ff.w3.weight\", \"blocks.8.ln1.weight\", \"blocks.8.ln2.weight\", \"blocks.9.attn.qkv.weight\", \"blocks.9.attn.out.weight\", \"blocks.9.attn.out.bias\", \"blocks.9.ff.w1.weight\", \"blocks.9.ff.w2.weight\", \"blocks.9.ff.w3.weight\", \"blocks.9.ln1.weight\", \"blocks.9.ln2.weight\", \"blocks.10.attn.qkv.weight\", \"blocks.10.attn.out.weight\", \"blocks.10.attn.out.bias\", \"blocks.10.ff.w1.weight\", \"blocks.10.ff.w2.weight\", \"blocks.10.ff.w3.weight\", \"blocks.10.ln1.weight\", \"blocks.10.ln2.weight\", \"blocks.11.attn.qkv.weight\", \"blocks.11.attn.out.weight\", \"blocks.11.attn.out.bias\", \"blocks.11.ff.w1.weight\", \"blocks.11.ff.w2.weight\", \"blocks.11.ff.w3.weight\", \"blocks.11.ln1.weight\", \"blocks.11.ln2.weight\". \n\tsize mismatch for token_embed.weight: copying a param with shape torch.Size([50258, 512]) from checkpoint, the shape in current model is torch.Size([50258, 768]).\n\tsize mismatch for blocks.0.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.0.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.0.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.0.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.0.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.0.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.0.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.0.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.1.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.1.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.1.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.1.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.1.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.1.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.1.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.1.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.2.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.2.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.2.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.2.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.2.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.2.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.2.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.2.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.3.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.3.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.3.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.3.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.3.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.3.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.3.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.3.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.4.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.4.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.4.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.4.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.4.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.4.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.4.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.4.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.5.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.5.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.5.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.5.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.5.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.5.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.5.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.5.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for ln_final.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([50258, 512]) from checkpoint, the shape in current model is torch.Size([50258, 768])."],"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for AdvancedLLM:\n\tMissing key(s) in state_dict: \"blocks.6.attn.qkv.weight\", \"blocks.6.attn.out.weight\", \"blocks.6.attn.out.bias\", \"blocks.6.ff.w1.weight\", \"blocks.6.ff.w2.weight\", \"blocks.6.ff.w3.weight\", \"blocks.6.ln1.weight\", \"blocks.6.ln2.weight\", \"blocks.7.attn.qkv.weight\", \"blocks.7.attn.out.weight\", \"blocks.7.attn.out.bias\", \"blocks.7.ff.w1.weight\", \"blocks.7.ff.w2.weight\", \"blocks.7.ff.w3.weight\", \"blocks.7.ln1.weight\", \"blocks.7.ln2.weight\", \"blocks.8.attn.qkv.weight\", \"blocks.8.attn.out.weight\", \"blocks.8.attn.out.bias\", \"blocks.8.ff.w1.weight\", \"blocks.8.ff.w2.weight\", \"blocks.8.ff.w3.weight\", \"blocks.8.ln1.weight\", \"blocks.8.ln2.weight\", \"blocks.9.attn.qkv.weight\", \"blocks.9.attn.out.weight\", \"blocks.9.attn.out.bias\", \"blocks.9.ff.w1.weight\", \"blocks.9.ff.w2.weight\", \"blocks.9.ff.w3.weight\", \"blocks.9.ln1.weight\", \"blocks.9.ln2.weight\", \"blocks.10.attn.qkv.weight\", \"blocks.10.attn.out.weight\", \"blocks.10.attn.out.bias\", \"blocks.10.ff.w1.weight\", \"blocks.10.ff.w2.weight\", \"blocks.10.ff.w3.weight\", \"blocks.10.ln1.weight\", \"blocks.10.ln2.weight\", \"blocks.11.attn.qkv.weight\", \"blocks.11.attn.out.weight\", \"blocks.11.attn.out.bias\", \"blocks.11.ff.w1.weight\", \"blocks.11.ff.w2.weight\", \"blocks.11.ff.w3.weight\", \"blocks.11.ln1.weight\", \"blocks.11.ln2.weight\". \n\tsize mismatch for token_embed.weight: copying a param with shape torch.Size([50258, 512]) from checkpoint, the shape in current model is torch.Size([50258, 768]).\n\tsize mismatch for blocks.0.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.0.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.0.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.0.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.0.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.0.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.0.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.0.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.1.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.1.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.1.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.1.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.1.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.1.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.1.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.1.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.2.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.2.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.2.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.2.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.2.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.2.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.2.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.2.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.3.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.3.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.3.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.3.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.3.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.3.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.3.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.3.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.4.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.4.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.4.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.4.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.4.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.4.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.4.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.4.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.5.attn.qkv.weight: copying a param with shape torch.Size([1536, 512]) from checkpoint, the shape in current model is torch.Size([2304, 768]).\n\tsize mismatch for blocks.5.attn.out.weight: copying a param with shape torch.Size([512, 512]) from checkpoint, the shape in current model is torch.Size([768, 768]).\n\tsize mismatch for blocks.5.attn.out.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.5.ff.w1.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.5.ff.w2.weight: copying a param with shape torch.Size([512, 1408]) from checkpoint, the shape in current model is torch.Size([768, 2048]).\n\tsize mismatch for blocks.5.ff.w3.weight: copying a param with shape torch.Size([1408, 512]) from checkpoint, the shape in current model is torch.Size([2048, 768]).\n\tsize mismatch for blocks.5.ln1.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for blocks.5.ln2.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for ln_final.weight: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([768]).\n\tsize mismatch for head.weight: copying a param with shape torch.Size([50258, 512]) from checkpoint, the shape in current model is torch.Size([50258, 768]).","output_type":"error"}],"execution_count":11},{"cell_type":"markdown","source":"## 💬 Test Your Model\n\nInteractive testing with your trained models","metadata":{}},{"cell_type":"code","source":"# Load and test trained model\nprint(\"💬 Model Testing Interface\")\nprint(\"=\"*60)\n\n# List available models\nif os.path.exists(KAGGLE_MODEL_DIR):\n    models = sorted([f for f in os.listdir(KAGGLE_MODEL_DIR) if f.endswith('_final.pt')])\n    \n    if models:\n        print(\"\\n📁 Available models:\")\n        for i, model in enumerate(models):\n            model_path = os.path.join(KAGGLE_MODEL_DIR, model)\n            size_gb = os.path.getsize(model_path) / (1024**3)\n            print(f\"  {i+1}. {model} ({size_gb:.2f}GB)\")\n        \n        # Load the first model (or modify to choose)\n        model_path = os.path.join(KAGGLE_MODEL_DIR, models[0])\n        print(f\"\\n📂 Loading: {model_path}\")\n        \n        inference = OfflineInference(model_path)\n        session_id = inference.create_session()\n        \n        # Test with sample prompts\n        test_prompts = [\n            \"What is artificial intelligence?\",\n            \"Explain machine learning in simple terms.\",\n            \"How does a neural network work?\",\n            \"What are the benefits of deep learning?\"\n        ]\n        \n        print(\"\\n\" + \"=\"*60)\n        print(\"🧪 Testing Model Responses\")\n        print(\"=\"*60 + \"\\n\")\n        \n        for prompt in test_prompts:\n            print(f\"👤 User: {prompt}\")\n            response, _ = inference.chat(\n                prompt, \n                session_id=session_id,\n                max_new_tokens=100,\n                temperature=0.7\n            )\n            print(f\"🤖 Model: {response}\")\n            print(\"-\" * 40)\n            \n        # Interactive chat (uncomment to enable)\n        # print(\"\\n💬 Interactive Chat (type 'quit' to exit)\")\n        # print(\"=\"*60)\n        # while True:\n        #     user_input = input(\"\\n👤 You: \")\n        #     if user_input.lower() in ['quit', 'exit', 'q']:\n        #         break\n        #     response, _ = inference.chat(user_input, session_id=session_id)\n        #     print(f\"🤖 Model: {response}\")\n        \n    else:\n        print(\"❌ No trained models found. Please train a model first.\")\nelse:\n    print(\"❌ Model directory doesn't exist. Please train a model first.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:13:36.089799Z","iopub.execute_input":"2025-10-25T11:13:36.090134Z","iopub.status.idle":"2025-10-25T11:13:42.784658Z","shell.execute_reply.started":"2025-10-25T11:13:36.090087Z","shell.execute_reply":"2025-10-25T11:13:42.783917Z"}},"outputs":[{"name":"stdout","text":"💬 Model Testing Interface\n============================================================\n\n📁 Available models:\n  1. cyberdyne-quick-1M_final.pt (0.17GB)\n  2. cyberdyne-standard-5M_final.pt (0.35GB)\n\n📂 Loading: /kaggle/working/models/cyberdyne-quick-1M_final.pt\n📂 Loading model from: /kaggle/working/models/cyberdyne-quick-1M_final.pt\n💬 Inference engine initialized on cuda\n✅ Offline inference ready!\n\n============================================================\n🧪 Testing Model Responses\n============================================================\n\n👤 User: What is artificial intelligence?\n🤖 Model: , and is then thought to be a high @-@ wing , in which the body is a few @-@ year @-@ old black @-@ down body . The entire <unk> is a major @-@ class body , with a single time that is a red @-@ of @-@ a @-@ hour and a @-@ point <unk> @-@ old @-@ old . The two @-@ based <unk> @-@\n----------------------------------------\n👤 User: Explain machine learning in simple terms.\n🤖 Model: : the <unk> @-@ <unk> .\n----------------------------------------\n👤 User: How does a neural network work?\n🤖 Model: for their own .\n----------------------------------------\n👤 User: What are the benefits of deep learning?\n🤖 Model: .\n----------------------------------------\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"## 🧹 Cleanup and Optimization\n\nManage disk space and optimize cache","metadata":{}},{"cell_type":"code","source":"# Cleanup utilities\ndef cleanup_cache(cache_dir=KAGGLE_CACHE_DIR, keep_gb=10):\n    \"\"\"Clean up cache directory to free disk space.\"\"\"\n    print(f\"🧹 Cleaning cache directory: {cache_dir}\")\n    \n    if not os.path.exists(cache_dir):\n        print(\"❌ Cache directory doesn't exist\")\n        return\n    \n    # Get current size\n    total_size = 0\n    file_list = []\n    \n    for dirpath, dirnames, filenames in os.walk(cache_dir):\n        for filename in filenames:\n            filepath = os.path.join(dirpath, filename)\n            size = os.path.getsize(filepath)\n            total_size += size\n            file_list.append((filepath, size, os.path.getmtime(filepath)))\n    \n    current_gb = total_size / (1024**3)\n    print(f\"📊 Current cache size: {current_gb:.2f}GB\")\n    \n    if current_gb <= keep_gb:\n        print(f\"✅ Cache is within limit ({keep_gb}GB)\")\n        return\n    \n    # Sort by modification time (oldest first)\n    file_list.sort(key=lambda x: x[2])\n    \n    # Remove oldest files until we're under the limit\n    removed_size = 0\n    removed_count = 0\n    target_size = keep_gb * (1024**3)\n    \n    for filepath, size, _ in file_list:\n        if total_size - removed_size <= target_size:\n            break\n        \n        try:\n            os.remove(filepath)\n            removed_size += size\n            removed_count += 1\n        except Exception as e:\n            print(f\"⚠️ Failed to remove {filepath}: {e}\")\n    \n    freed_gb = removed_size / (1024**3)\n    print(f\"✅ Removed {removed_count} files, freed {freed_gb:.2f}GB\")\n    \n    # Final status\n    monitor_resources()\n\ndef optimize_models(model_dir=KAGGLE_MODEL_DIR, keep_best=3):\n    \"\"\"Keep only the best models to save space.\"\"\"\n    print(f\"🗂️ Optimizing model directory: {model_dir}\")\n    \n    if not os.path.exists(model_dir):\n        print(\"❌ Model directory doesn't exist\")\n        return\n    \n    models = [f for f in os.listdir(model_dir) if f.endswith('.pt')]\n    \n    if len(models) <= keep_best:\n        print(f\"✅ Only {len(models)} models, no cleanup needed\")\n        return\n    \n    # Sort by modification time (keep newest)\n    models.sort(\n        key=lambda x: os.path.getmtime(os.path.join(model_dir, x)),\n        reverse=True\n    )\n    \n    # Remove older models\n    for model in models[keep_best:]:\n        model_path = os.path.join(model_dir, model)\n        size_gb = os.path.getsize(model_path) / (1024**3)\n        os.remove(model_path)\n        print(f\"🗑️ Removed {model} ({size_gb:.2f}GB)\")\n    \n    print(f\"✅ Kept {keep_best} best models\")\n\n# Run cleanup\nprint(\"🧹 Cleanup and Optimization\")\nprint(\"=\"*60)\n\n# Check current status\nmonitor_resources()\n\n# Clean cache if needed (keep 10GB)\n# cleanup_cache(keep_gb=10)\n\n# Optimize models (keep best 3)\n# optimize_models(keep_best=3)\n\n# Clear GPU cache\nif device == 'cuda':\n    torch.cuda.empty_cache()\n    print(\"✅ GPU cache cleared\")\n\n# Garbage collection\ngc.collect()\nprint(\"✅ Python garbage collected\")\n\n# Final status\nprint(\"\\n📊 Final resource status:\")\nmonitor_resources()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:10:50.955962Z","iopub.status.idle":"2025-10-25T11:10:50.956253Z","shell.execute_reply.started":"2025-10-25T11:10:50.956094Z","shell.execute_reply":"2025-10-25T11:10:50.956129Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 📊 Training Analysis & Visualization\n\nAnalyze training history and performance","metadata":{}},{"cell_type":"code","source":"import json\nimport matplotlib.pyplot as plt\n\ndef analyze_training_history(model_name):\n    \"\"\"Analyze and visualize training history.\"\"\"\n    history_path = os.path.join(KAGGLE_MODEL_DIR, f\"{model_name}_history.json\")\n    \n    if not os.path.exists(history_path):\n        print(f\"❌ No history found for {model_name}\")\n        return\n    \n    with open(history_path, 'r') as f:\n        history = json.load(f)\n    \n    print(f\"\\n📊 Training Analysis: {model_name}\")\n    print(\"=\"*60)\n    print(f\"Dataset: {history['dataset']}\")\n    print(f\"Epochs: {history['epochs']}\")\n    print(f\"Batch size: {history['batch_size']} × {history['gradient_accumulation_steps']} accumulation\")\n    print(f\"Duration: {history['duration']/3600:.2f} hours\")\n    print(f\"Final loss: {history['epoch_losses'][-1]:.4f}\")\n    \n    # Plot training curves\n    if history['losses']:\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n        \n        # Loss over steps\n        steps = [l['step'] for l in history['losses']]\n        losses = [l['loss'] for l in history['losses']]\n        ax1.plot(steps, losses)\n        ax1.set_xlabel('Steps')\n        ax1.set_ylabel('Loss')\n        ax1.set_title('Training Loss')\n        ax1.grid(True, alpha=0.3)\n        \n        # Learning rate over steps\n        lrs = [l['lr'] for l in history['losses']]\n        ax2.plot(steps, lrs)\n        ax2.set_xlabel('Steps')\n        ax2.set_ylabel('Learning Rate')\n        ax2.set_title('Learning Rate Schedule')\n        ax2.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        # Epoch summary\n        if history['epoch_losses']:\n            print(\"\\n📈 Epoch Losses:\")\n            for i, loss in enumerate(history['epoch_losses'], 1):\n                print(f\"  Epoch {i}: {loss:.4f}\")\n\n# Analyze available models\nif os.path.exists(KAGGLE_MODEL_DIR):\n    history_files = [f for f in os.listdir(KAGGLE_MODEL_DIR) if f.endswith('_history.json')]\n    \n    if history_files:\n        print(\"📊 Available training histories:\")\n        for history_file in history_files:\n            model_name = history_file.replace('_history.json', '')\n            print(f\"  - {model_name}\")\n            # Uncomment to analyze\n            # analyze_training_history(model_name)\n    else:\n        print(\"❌ No training histories found\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T11:10:50.957733Z","iopub.status.idle":"2025-10-25T11:10:50.958241Z","shell.execute_reply.started":"2025-10-25T11:10:50.958042Z","shell.execute_reply":"2025-10-25T11:10:50.958059Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 📝 Summary & Next Steps\n\n### ✅ What You've Accomplished\n- Configured optimized dataset caching for Kaggle's 50GB SSD\n- Implemented gradient accumulation for effective large batch training\n- Added checkpoint resuming for interrupted training sessions\n- Created configurations for different training scales (1M, 5M, 20M+ samples)\n- Enabled memory optimizations (gradient checkpointing, mixed precision)\n- Built resource monitoring and cleanup utilities\n\n### 🎯 Next Steps\n1. **Scale Up**: Try the large-scale Pile configuration with 20M+ samples\n2. **Fine-tune**: Load a checkpoint and fine-tune on specific datasets\n3. **Experiment**: Try different model architectures (layers, heads, embedding size)\n4. **Deploy**: Export models for inference in production\n5. **Benchmark**: Compare performance across different configurations\n\n### 💡 Tips for Kaggle\n- Always monitor disk usage with `monitor_resources()`\n- Use gradient accumulation for larger effective batch sizes\n- Enable gradient checkpointing for larger models\n- Clean cache periodically with `cleanup_cache()`\n- Save checkpoints frequently for long training runs\n- Use mixed precision training for 2x speedup\n\n### 🔗 Resources\n- [Hugging Face Datasets](https://huggingface.co/datasets)\n- [PyTorch Documentation](https://pytorch.org/docs/)\n- [The Pile Dataset](https://pile.eleuther.ai/)\n\nHappy Training! 🚀","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}